{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import unet2d\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_dir', type=str, default='/raid/COVID-19/CT-severity/processed/dataset/')\n",
    "parser.add_argument('--include', type=str, default='medseg_1')\n",
    "\n",
    "parser.add_argument('--restore_dir', type=str, \n",
    "                    default='/raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/em/bias_-7_scale_1.5/')\n",
    "parser.add_argument('--checkpoint', type=str, default='49')\n",
    "parser.add_argument('--output_file', type=str, default='ablation.csv')\n",
    "\n",
    "parser.add_argument('--device', type=str, default='0')\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "\n",
    "net = unet2d.unet2d()\n",
    "parser = net.add_to_parser(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers = 5\n",
      "fc_nodes = [1024]\n",
      "finetune_scope = finetune\n",
      "filter_size = 3\n",
      "input_dir = /raid/COVID-19/CT-severity/processed/dataset/\n",
      "keep_prob = 1.0\n",
      "output_file = ablation.csv\n",
      "n_class = 3\n",
      "dice_smooth = 1\n",
      "batch_size = 16\n",
      "input_shape = [256, 256, 7]\n",
      "checkpoint = 49\n",
      "up_layers = 0\n",
      "pool_size = 2\n",
      "device = 5\n",
      "restore_dir = /raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/em/bias_-7_scale_1.5/\n",
      "include = medseg_1\n",
      "fc_drop = [0.25]\n",
      "features_root = 32\n"
     ]
    }
   ],
   "source": [
    "if sys.argv[0] != 'ablation_em_external.py':\n",
    "    args = parser.parse_args(['--device', '5', '--n_class', '3'])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "for k in vars(args):\n",
    "    print (k, '=', vars(args)[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/em/bias_-7_scale_1.5/49\n"
     ]
    }
   ],
   "source": [
    "# build network\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device\n",
    "tf.reset_default_graph()\n",
    "model = unet2d.unet2d()\n",
    "model.from_args(args)\n",
    "model.build_unet_mask()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# restore\n",
    "loader = tf.train.Saver()\n",
    "loader.restore(sess, os.path.join(args.restore_dir, args.checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_dir, cohort, exclude_set = []):\n",
    "    inds = [int(os.path.basename(s)[:-len('.npz')]) for s in glob.glob(os.path.join(input_dir, '*.npz'))]\n",
    "    inds = [d for d in inds if d not in exclude_set]\n",
    "    \n",
    "    if len(inds) == 0:\n",
    "        return None\n",
    "    \n",
    "    dataset = {}\n",
    "    for index in inds:\n",
    "        f = np.load(os.path.join(input_dir, '%d.npz'%index))\n",
    "        for k in f:\n",
    "            if k not in dataset:\n",
    "                dataset[k] = []\n",
    "            dataset[k].append(f[k])\n",
    "    \n",
    "    for k in dataset:\n",
    "        dataset[k] = np.concatenate(dataset[k])\n",
    "    dataset['cohort'] = np.array([cohort] * len(dataset[dataset.keys()[0]]))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medseg_1']\n",
      "['medseg_1']\n"
     ]
    }
   ],
   "source": [
    "# get studying cohorts\n",
    "include_cohort = args.include.split(',')\n",
    "print (include_cohort)\n",
    "cohorts = [os.path.basename(s) for s in glob.glob(os.path.join(args.input_dir, '*')) if os.path.basename(s) in include_cohort and os.path.isdir(s)]\n",
    "print (cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading medseg_1\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "data_list = []\n",
    "for cohort in cohorts:\n",
    "    print ('loading %s'%cohort)\n",
    "    d = load_dataset(os.path.join(args.input_dir, cohort, 'npzs', 'with_unet_pred'), cohort)\n",
    "    if d is not None:\n",
    "        data_list.append(d)\n",
    "dataset = {}\n",
    "for k in data_list[0]:\n",
    "    dataset[k] = np.concatenate([d[k] for d in data_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# load patient information\n",
    "patient_infos = {}\n",
    "for cohort in cohorts:\n",
    "    if os.path.exists(os.path.join(args.input_dir, cohort, 'patient_types.npy')):\n",
    "        info = np.load(os.path.join(args.input_dir, cohort, 'patient_types.npy'), allow_pickle=True).item()\n",
    "        for k in info:\n",
    "            info[k]['cohort'] = cohort\n",
    "        patient_infos.update(info)\n",
    "\n",
    "# load exclusion\n",
    "df = []\n",
    "for cohort in cohorts:\n",
    "    if os.path.exists(os.path.join(args.input_dir, cohort, 'mrn_train.csv')):\n",
    "        df.append(pd.read_csv(os.path.join(args.input_dir, cohort, 'mrn_train.csv')))\n",
    "if len(df) == 0:\n",
    "    mrn_to_exclude = []\n",
    "else:\n",
    "    mrn_to_exclude = list(pd.concat(df, ignore_index=True).filename)\n",
    "print (mrn_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_data_mrn(mrn, dataset, patient_infos):\n",
    "    patient = {}\n",
    "    \n",
    "    inds = np.where(dataset['mrns'] == mrn)[0]\n",
    "    \n",
    "    for k in dataset:\n",
    "        patient[k] = dataset[k][inds]\n",
    "    if mrn in patient_infos:\n",
    "        patient['info'] = patient_infos[mrn]\n",
    "    else:\n",
    "        patient['info'] = None\n",
    "        \n",
    "    return patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_dice(img, label):\n",
    "    return 2 * np.sum(img * label, dtype = np.float32) / (np.sum(img) + np.sum(label) + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n",
      "144,304,concatenating pred\n"
     ]
    }
   ],
   "source": [
    "# predict all\n",
    "preds = []\n",
    "imgs = dataset['img']\n",
    "masks = np.where(dataset['pred'] > 0.5, 1, 0)\n",
    "print (imgs.shape[0])\n",
    "for ibatch in range(0, imgs.shape[0], args.batch_size):\n",
    "    if (ibatch // args.batch_size + 1) % 10 == 0:\n",
    "        print (ibatch, end=',')\n",
    "    batch_x = imgs[ibatch:ibatch+args.batch_size]\n",
    "    batch_mask = masks[ibatch:ibatch+args.batch_size]\n",
    "    pred = sess.run(model.pred, {model.X: batch_x, model.mask: batch_mask, model.phase: 0})\n",
    "\n",
    "    preds.append(pred)\n",
    "print ('concatenating pred')\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "dataset['pred_type'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the type labels\n",
    "dataset['pred_final'] = np.copy(dataset['pred'])\n",
    "dataset['pred_final'][dataset['pred_type'] > 0.5] = 2\n",
    "\n",
    "th = (1024 - 200) / 110.0\n",
    "dataset['pred_th'] = np.copy(dataset['pred'])\n",
    "con_th = np.where(imgs[..., [3]] > th, 1, 0) * masks[...,[0]]\n",
    "dataset['pred_th'][con_th == 1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6226212175268134\n",
      "0.5300540329233202\n"
     ]
    }
   ],
   "source": [
    "# calculate dice\n",
    "label_con = np.where(dataset['label'] == 2, 1, 0)\n",
    "pred_con = np.where(dataset['pred_final'] == 2, 1, 0)\n",
    "pred_con_th = np.where(dataset['pred_th'] == 2, 1, 0)\n",
    "dice_pred_con = hard_dice(label_con, pred_con)\n",
    "print (dice_pred_con)\n",
    "\n",
    "# threshold dice\n",
    "dice_th_con = hard_dice(label_con, con_th)\n",
    "print (dice_th_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5917871533538753 0.4214268784072055\n",
      "0.9885767478048239 0.994220295654107\n"
     ]
    }
   ],
   "source": [
    "masks = np.where(dataset['label'] > 0, 1, 0)\n",
    "sensitivity = np.sum(masks * pred_con * label_con, dtype=np.float32) / np.sum(masks * label_con)\n",
    "sensitivity_th = np.sum(masks * pred_con_th * label_con, dtype=np.float32) / np.sum(masks * label_con)\n",
    "print (sensitivity, sensitivity_th)\n",
    "\n",
    "specificity = np.sum((masks-pred_con) * (masks-masks*label_con), dtype=np.float32) / np.sum(masks-masks*label_con)\n",
    "specificity_th = np.sum((masks-pred_con_th) * (masks-masks*label_con), dtype=np.float32) / np.sum(masks-masks*label_con)\n",
    "print (specificity, specificity_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'dice': dice_pred_con, 'sensitivity': sensitivity, 'specificity': specificity}, index=[0])\n",
    "df.to_csv(os.path.join(args.restore_dir, 'log', args.output_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
