{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import unet2d\n",
    "import augmentor2d\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_dir', type=str, \n",
    "                    default='/raid/COVID-19/CT-severity/processed/Iran-2020-04-01-with-annotation/256x256x7channels_preprocessed_with_mask/weak/')\n",
    "parser.add_argument('--exclude', type=int, nargs='+', default=[0,100])\n",
    "parser.add_argument('--output_dir', type=str, \n",
    "                    default='/raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/em/')\n",
    "parser.add_argument('--tag', type=str, default='debug')\n",
    "parser.add_argument('--restore_dir', type=str, \n",
    "                    default='/raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/segmentation/focal_0_area_0/')\n",
    "parser.add_argument('--checkpoint', type=str, default='199')\n",
    "\n",
    "parser.add_argument('--device', type=str, default='0')\n",
    "parser.add_argument('--nepochs', type=int, nargs=1, default=[50], help = 'training of the last conv layer, training of fine tune layer, training of all layer')\n",
    "parser.add_argument('--lr', type=float, nargs=1, default=[0.0005])\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--finetune_layers', type=int, default=9)\n",
    "parser.add_argument('--balance', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--bias', type=float, default=-7)\n",
    "parser.add_argument('--scale', type=float, default=0.5)\n",
    "\n",
    "net = unet2d.unet2d()\n",
    "parser = net.add_to_parser(parser)\n",
    "\n",
    "aug = augmentor2d.multi_thread_augmentor()\n",
    "parser = aug.add_to_parser(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoom_range = [0.9, 1.1]\n",
      "keep_prob = 1.0\n",
      "n_process = 8\n",
      "filter_size = 3\n",
      "bias = -7.5\n",
      "exclude = [0, 100]\n",
      "rotation_range = 90\n",
      "layers = 5\n",
      "scale = 0.5\n",
      "dice_smooth = 1\n",
      "checkpoint = 199\n",
      "up_layers = 0\n",
      "lr = [0.0005]\n",
      "output_dir = /raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/em/\n",
      "fc_drop = [0.25]\n",
      "finetune_scope = finetune\n",
      "vertical_flip = 1\n",
      "n_class = 3\n",
      "input_shape = [256, 256, 7]\n",
      "batch_size = 16\n",
      "nepochs = [50]\n",
      "device = 1\n",
      "restore_dir = /raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/segmentation/focal_0_area_0/\n",
      "fc_nodes = [1024]\n",
      "features_root = 32\n",
      "width_shift_range = 0.1\n",
      "input_dir = /raid/COVID-19/CT-severity/processed/Iran-2020-04-01-with-annotation/256x256x7channels_preprocessed_with_mask/weak/\n",
      "pool_size = 2\n",
      "tag = debug\n",
      "height_shift_range = 0.1\n",
      "horizontal_flip = 1\n",
      "finetune_layers = 9\n",
      "balance = 1\n"
     ]
    }
   ],
   "source": [
    "if sys.argv[0] != 'train_em.py':\n",
    "    # background, GGO, consolidation\n",
    "    args = parser.parse_args(['--device', '1', '--n_class', '3', '--bias', '-7.5', '--scale', '0.5'])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "for k in vars(args):\n",
    "    print (k, '=', vars(args)[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/segmentation/focal_0_area_0/199\n"
     ]
    }
   ],
   "source": [
    "# build network\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.device\n",
    "tf.reset_default_graph()\n",
    "model = unet2d.unet2d()\n",
    "model.from_args(args)\n",
    "model.build_unet_mask()\n",
    "total_loss = model.dice_loss\n",
    "\n",
    "all_unet_layers = model.down_layer_vars + model.up_layer_vars\n",
    "all_unet_vars = list(np.concatenate(all_unet_layers))\n",
    "finetune_unet_vars = list(np.concatenate(all_unet_layers[-args.finetune_layers:]))\n",
    "new_vars = [v for v in tf.trainable_variables() if v not in all_unet_vars]\n",
    "bn_vars = [v for v in tf.global_variables() if v not in tf.trainable_variables()]\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "lr = tf.placeholder(tf.float32, name = 'lr')\n",
    "trainers = []\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "#     trainers.append(optimizer.minimize(total_loss, var_list = new_vars))\n",
    "#     trainers.append(optimizer.minimize(total_loss, var_list = new_vars + finetune_unet_vars))\n",
    "    trainers.append(optimizer.minimize(total_loss, var_list = new_vars + all_unet_vars))\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=args.nepochs[-1])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# restore\n",
    "loader = tf.train.Saver()\n",
    "loader.restore(sess, os.path.join(args.restore_dir, args.checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(annotations, imgs, labels, args):\n",
    "    # remove the -1 in the annotations\n",
    "    sum_annotations = annotations.sum(1)\n",
    "    inds_include = np.where(sum_annotations >= 0)[0]\n",
    "    \n",
    "    # leave only the first two annotations columns\n",
    "    annotations = annotations[:, :2]\n",
    "    \n",
    "    # discard the ones with label but no annotation, or with annotation but no label\n",
    "    sum_labels = np.sum(labels, (1,2,3))\n",
    "    sum_annotations = np.sum(annotations, 1)\n",
    "    inds_exclude_1 = np.where((sum_labels > 0) & (sum_annotations == 0))[0]\n",
    "    inds_exclude_2 = np.where((sum_labels == 0) & (sum_annotations > 0))[0]\n",
    "    \n",
    "    # select the training cohort\n",
    "    inds = [i for i in inds_include if i not in np.concatenate((inds_exclude_1, inds_exclude_2))]\n",
    "    \n",
    "    return annotations[inds], imgs[inds], labels[inds], inds\n",
    "\n",
    "def prepare_label_and_annotations(annotations, labels, imgs, th = (1024-200)/110, dice_weight = 1):\n",
    "    '''\n",
    "    only label the consolidation\n",
    "    '''\n",
    "    label_con = np.copy(labels)\n",
    "    classes = np.zeros(len(annotations), np.float32)\n",
    "    # air\n",
    "    inds = np.where(annotations.sum(1) == 0)[0]\n",
    "    classes[inds] = 0\n",
    "    label_con[inds, ...] = 0\n",
    "    # GGO\n",
    "    inds_ggo = np.where(annotations[:, 0] == 1)[0]\n",
    "    classes[inds_ggo] = 1\n",
    "    label_con[inds_ggo, ...] = 0\n",
    "    # GGO + consolidation    \n",
    "    inds_mix = np.where(annotations[:, 1] == 1)[0]\n",
    "    for i in inds_mix:\n",
    "        img_mix = imgs[i, ..., 3]\n",
    "        label_con[i, img_mix < th, 0] = 0     # only preserve larger than threshold\n",
    "    classes[inds_mix] = 2\n",
    "    \n",
    "    return classes, label_con, inds_ggo, inds_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_index(classes, balance = True):\n",
    "    if not balance:\n",
    "        inds = np.arange(len(classes))\n",
    "        np.random.shuffle(inds)\n",
    "        return inds\n",
    "    \n",
    "    cnt_per_class = [np.count_nonzero(classes == s) for s in np.unique(classes)]\n",
    "    target_cnt_per_class = max(cnt_per_class)\n",
    "    sample_inds = []\n",
    "    for iclass in range(len(cnt_per_class)):\n",
    "        base_inds = list(np.where(classes == iclass)[0])\n",
    "        n_rep = int(np.ceil(target_cnt_per_class / float(len(base_inds))))\n",
    "        inds = []\n",
    "        for i in range(n_rep):\n",
    "            np.random.shuffle(base_inds)\n",
    "            inds += base_inds\n",
    "        sample_inds.append(inds[:target_cnt_per_class])\n",
    "    \n",
    "    return np.array(sample_inds).T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "list_filename = glob.glob(os.path.join(args.input_dir, '*.npz'))\n",
    "mrns = []\n",
    "imgs = []\n",
    "labels = []\n",
    "annotations = []\n",
    "for filename in list_filename:\n",
    "    dataset = os.path.basename(filename)[:-len('.npz')]\n",
    "    if int(dataset) in args.exclude:\n",
    "        continue\n",
    "    f = np.load(filename)\n",
    "    \n",
    "    mrns.append(f['mrn'])\n",
    "    imgs.append(f['img'])\n",
    "    labels.append(f['label'])\n",
    "    annotations.append(f['annotation'])\n",
    "\n",
    "mrns = np.concatenate(mrns)\n",
    "imgs = np.concatenate(imgs)\n",
    "labels = np.concatenate(labels)\n",
    "annotations = np.concatenate(annotations)\n",
    "\n",
    "annotations, imgs, labels, inds = clean_data(annotations, imgs, labels, args)\n",
    "classes, labels_con, inds_ggo, inds_mix = prepare_label_and_annotations(annotations, labels, imgs)\n",
    "inds_one_type = np.array([i for i in np.arange(len(labels)) if i not in inds_mix])\n",
    "labels = np.concatenate((labels_con, labels), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "mrn_used = mrns[inds]\n",
    "mrn_unique = np.unique(mrn_used)\n",
    "print (len(mrn_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_types = []\n",
    "for mrn in mrn_unique:\n",
    "    sub_inds = np.where(mrn_used == mrn)[0]\n",
    "    sub_annotation = annotations[sub_inds]\n",
    "    sum_annotation = np.sum(sub_annotation, 0)\n",
    "    patient_types.append(np.argmax(sum_annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(patient_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_summary(summary_writer, loss_vals, loss_names, global_step):\n",
    "    summary = tf.Summary()\n",
    "    for val, name in zip(loss_vals, loss_names):\n",
    "        summary.value.add(tag = name, simple_value = val)\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "def add_to_record(records, loss_vals, loss_names, phase):\n",
    "    for val, name in zip(loss_vals, loss_names):\n",
    "        tag = '%s_%s'%(name, phase)\n",
    "        if not tag in records:\n",
    "            records[tag] = [val]\n",
    "        else:\n",
    "            records[tag].append(val)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation dataset\n",
    "valid_path = '/raid/COVID-19/CT-severity/processed/dataset/medseg_1/npzs/with_unet_pred/0.npz'\n",
    "with np.load(valid_path) as f:\n",
    "    valid_imgs = f['img']\n",
    "    valid_labels = f['label']\n",
    "    valid_preds = np.where(f['pred'] > 0.5, 1, 0)\n",
    "    valid_lungs = f['lung']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dice(sess, model, imgs, labels, masks, args):\n",
    "    preds = []\n",
    "    \n",
    "    for ibatch in range(0, imgs.shape[0], args.batch_size):\n",
    "        batch_x = imgs[ibatch:ibatch+args.batch_size]\n",
    "        batch_mask = masks[ibatch:ibatch+args.batch_size]\n",
    "        preds.append(sess.run(model.pred, {model.X: batch_x, model.mask: batch_mask, model.phase: 0}))\n",
    "    preds = np.concatenate(preds)\n",
    "    preds_con = np.where(preds > 0.5, 1, 0)\n",
    "    labels_con = np.where(labels == 2, 1, 0)\n",
    "    \n",
    "    dice_val = 2 * np.sum(preds_con * labels_con, dtype = np.float32) / (np.sum(preds_con) + np.sum(labels_con))\n",
    "    \n",
    "    return dice_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "summary_dir = os.path.join(args.output_dir, args.tag, 'log')\n",
    "datasets = ['train']\n",
    "writers = {}\n",
    "for dataset in datasets:\n",
    "    writers[dataset] = tf.summary.FileWriter(os.path.join(summary_dir, dataset), sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(preds, imgs, masks, class_labels, bias, scale):\n",
    "    # prior probabilities of being consolidation\n",
    "    if scale <= 0:\n",
    "        pred_cons = bias\n",
    "    else:\n",
    "        pred_cons = 1 / (1 + np.exp(-(imgs[..., [3]] + bias) * scale))\n",
    "    pred_cons *= masks\n",
    "    preds += pred_cons * 1\n",
    "    preds[class_labels != 2, ...] = 0\n",
    "    \n",
    "    return np.where(preds > 0.5, 1, 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /raid/COVID-19/CT-severity/results/Iran-2020-04-01-with-annotation/unet2d_256x256x7_mask/segmentation/focal_0_area_0/199\n"
     ]
    }
   ],
   "source": [
    "loader = tf.train.Saver()\n",
    "loader.restore(sess, os.path.join(args.restore_dir, args.checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 9/85: dice_loss = 0.105809\n",
      "0, 19/85: dice_loss = 0.251373\n",
      "0, 29/85: dice_loss = 0.103002\n",
      "0, 39/85: dice_loss = 0.360727\n",
      "0, 49/85: dice_loss = 0.332213\n",
      "0, 59/85: dice_loss = 0.0859153\n",
      "0, 69/85: dice_loss = 0.204161\n",
      "0, 79/85: dice_loss = 0.235114\n",
      "0, train: dice = 0.22482\n",
      "1, 9/85: dice_loss = 0.0962506\n",
      "1, 19/85: dice_loss = 0.400404\n",
      "1, 29/85: dice_loss = 0.105549\n",
      "1, 39/85: dice_loss = 0.0786126\n",
      "1, 49/85: dice_loss = 0.307789\n",
      "1, 59/85: dice_loss = 0.0938705\n",
      "1, 69/85: dice_loss = 0.13732\n",
      "1, 79/85: dice_loss = 0.135245\n",
      "1, train: dice = 0.529336\n",
      "2, 9/85: dice_loss = 0.14865\n",
      "2, 19/85: dice_loss = 0.0702805\n",
      "2, 29/85: dice_loss = 0.157652\n",
      "2, 39/85: dice_loss = 0.047279\n",
      "2, 49/85: dice_loss = 0.135658\n",
      "2, 59/85: dice_loss = 0.117952\n",
      "2, 69/85: dice_loss = 0.0843778\n",
      "2, 79/85: dice_loss = 0.426083\n",
      "2, train: dice = 0.516865\n",
      "3, 9/85: dice_loss = 0.100146\n",
      "3, 19/85: dice_loss = 0.320772\n",
      "3, 29/85: dice_loss = 0.386803\n",
      "3, 39/85: dice_loss = 0.199513\n",
      "3, 49/85: dice_loss = 0.26929\n",
      "3, 59/85: dice_loss = 0.241563\n",
      "3, 69/85: dice_loss = 0.59799\n",
      "3, 79/85: dice_loss = 0.182668\n",
      "3, train: dice = 0.00201053\n",
      "4, 9/85: dice_loss = 0.432392\n",
      "4, 19/85: dice_loss = 0.163409\n",
      "4, 29/85: dice_loss = 0.0881928\n",
      "4, 39/85: dice_loss = 0.591395\n",
      "4, 49/85: dice_loss = 0.108668\n",
      "4, 59/85: dice_loss = 0.266707\n",
      "4, 69/85: dice_loss = 0.172033\n",
      "4, 79/85: dice_loss = 0.115462\n",
      "4, train: dice = 0.0698586\n",
      "5, 9/85: dice_loss = 0.419718\n",
      "5, 19/85: dice_loss = 0.112542\n",
      "5, 29/85: dice_loss = 0.289908\n",
      "5, 39/85: dice_loss = 0.336178\n",
      "5, 49/85: dice_loss = 0.237079\n",
      "5, 59/85: dice_loss = 0.987339\n",
      "5, 69/85: dice_loss = 0.017181\n",
      "5, 79/85: dice_loss = 3.57628e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cbd18c9940bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         _, dice_val = sess.run([current_trainer, total_loss], \n\u001b[0;32m---> 40\u001b[0;31m             {model.X: batch_x, model.Y: batch_z, model.mask: batch_mask, model.phase: 1, lr: current_lr})\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dufan.wu/miniconda3/envs/test_tfgpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dufan.wu/miniconda3/envs/test_tfgpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dufan.wu/miniconda3/envs/test_tfgpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dufan.wu/miniconda3/envs/test_tfgpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dufan.wu/miniconda3/envs/test_tfgpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dufan.wu/miniconda3/envs/test_tfgpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aug = augmentor2d.multi_thread_augmentor()\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for epoch in range(args.nepochs[-1]):\n",
    "    inds = get_sample_index(classes, args.balance)\n",
    "    nbatches = len(inds) // args.batch_size\n",
    "    \n",
    "    # learning rate scheme\n",
    "    for k, epoch_th in enumerate(args.nepochs):\n",
    "        if epoch < epoch_th:\n",
    "            current_lr = args.lr[k]\n",
    "            current_trainer = trainers[k]\n",
    "            break\n",
    "    \n",
    "    # get first batch\n",
    "    next_inds = inds[:args.batch_size]\n",
    "    aug.start_next_batch_2d(next_inds, imgs, labels)\n",
    "    batch_x, batch_labels = aug.get_results()\n",
    "    batch_y = batch_labels[..., [0]]\n",
    "    batch_mask = batch_labels[..., [1]]\n",
    "    batch_class = classes[next_inds]\n",
    "    for ibatch in range(1, len(inds), args.batch_size):\n",
    "        # start retrieving next batch\n",
    "        next_inds = inds[ibatch:ibatch+args.batch_size]\n",
    "        aug.start_next_batch_2d(next_inds, imgs, labels)\n",
    "        \n",
    "        # train with current batch\n",
    "        # first get the prediction\n",
    "        batch_pred = sess.run(model.pred, {model.X: batch_x, model.mask: batch_mask, model.phase: 0})\n",
    "        \n",
    "        # then generate labels according to the prediction\n",
    "        if epoch > -1:\n",
    "            batch_z = get_label(np.copy(batch_pred), batch_x, batch_mask, batch_class, args.bias, args.scale)\n",
    "        else:\n",
    "            batch_z = batch_y\n",
    "        \n",
    "        _, dice_val = sess.run([current_trainer, total_loss], \n",
    "            {model.X: batch_x, model.Y: batch_z, model.mask: batch_mask, model.phase: 1, lr: current_lr})\n",
    "        \n",
    "        # tensorboard\n",
    "        add_to_summary(writers['train'], [dice_val], ['dice_batch'], \n",
    "                       epoch * nbatches + ibatch // args.batch_size)\n",
    "        \n",
    "        # get next batch\n",
    "        batch_x, batch_labels = aug.get_results()\n",
    "        batch_y = batch_labels[..., [0]]\n",
    "        batch_mask = batch_labels[..., [1]]\n",
    "        batch_class = classes[next_inds]\n",
    "        \n",
    "        # print some information\n",
    "        if (ibatch // args.batch_size + 1) % 10 == 0:\n",
    "            print ('%d, %d/%d: dice_loss = %g'%\\\n",
    "                   (epoch, ibatch // args.batch_size, nbatches, dice_val))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "#     break\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        saver.save(sess, os.path.join(args.output_dir, args.tag, str(epoch)))\n",
    "    \n",
    "    # validation and testing\n",
    "    records = {}\n",
    "    \n",
    "    dice_val = evaluate_dice(sess, model, valid_imgs, valid_labels, valid_preds, args)\n",
    "\n",
    "    print ('%d, %s: dice = %g'%(epoch, dataset, dice_val))\n",
    "\n",
    "    add_to_record(records, [dice_val], ['dice'], 'train')\n",
    "    add_to_summary(writers[dataset], [dice_val], ['dice'], epoch * nbatches + ibatch // args.batch_size)\n",
    "    \n",
    "    for k in records:\n",
    "        records[k] = np.mean(records[k])\n",
    "    df = df.append(records, ignore_index=True)\n",
    "    df.to_csv(os.path.join(summary_dir, 'logs.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
