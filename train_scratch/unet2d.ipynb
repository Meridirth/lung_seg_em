{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "\n",
    "def corrupt(x):\n",
    "    return tf.multiply(x, tf.cast(tf.random_uniform(shape=tf.shape(x),\n",
    "                                               minval=0,\n",
    "                                               maxval=2,\n",
    "                                               dtype=tf.int32), tf.float32))\n",
    "\n",
    "def batch_relu(x, phase, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        return tf.cond(phase,  \n",
    "                lambda: tf.contrib.layers.batch_norm(x, is_training=True, decay=0.9, zero_debias_moving_mean=True,\n",
    "                                   center=False, updates_collections=None, scope='bnn'),  \n",
    "                lambda: tf.contrib.layers.batch_norm(x, is_training=False,  decay=0.9, zero_debias_moving_mean=True,\n",
    "                                   updates_collections=None, center=False, scope='bnn', reuse = True))  \n",
    "\n",
    "\n",
    "#########################################################################\n",
    "def weight_variable(shape):\n",
    "    #initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    n_input=shape[2]\n",
    "    initial= tf.random_uniform(shape,-1.0 / math.sqrt(n_input),1.0 / math.sqrt(n_input))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def weight_variable_devonc(shape):\n",
    "    #return tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    n_input=shape[2]\n",
    "    initial= tf.random_uniform(shape,-1.0 / math.sqrt(n_input),1.0 / math.sqrt(n_input))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.00001, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W,keep_prob_):\n",
    "    conv_2d = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.dropout(conv_2d, keep_prob_)\n",
    "\n",
    "def conv2d_stride(x, W,keep_prob_):\n",
    "    conv_2d = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    return tf.nn.dropout(conv_2d, keep_prob_)\n",
    "\n",
    "def deconv2d(x, W,stride):\n",
    "    x_shape = tf.shape(x)\n",
    "#     x_shape = x.shape\n",
    "    output_shape = tf.stack([x_shape[0], x_shape[1]*2, x_shape[2]*2,  x_shape[3]//2])\n",
    "    return tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding='VALID')\n",
    "\n",
    "def max_pool(x,n):\n",
    "    return tf.nn.max_pool(x, ksize=[1, n, n, 1], strides=[1, n, n, 1], padding='VALID')\n",
    "\n",
    "def crop_and_concat(x1,x2):\n",
    "    x1_shape = tf.shape(x1)\n",
    "    x2_shape = tf.shape(x2)\n",
    "#     x1_shape = x1.shape\n",
    "#     x2_shape = x2.shape\n",
    "    # offsets for the top left corner of the crop\n",
    "    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
    "    size = [-1, x2_shape[1], x2_shape[2], -1]\n",
    "    x1_crop = tf.slice(x1, offsets, size)\n",
    "    return tf.concat(axis=3, values=[x1_crop, x2])\n",
    "\n",
    "# def crop_add(x1,x2):\n",
    "#     x1_shape = tf.shape(x1)\n",
    "#     x2_shape = tf.shape(x2)\n",
    "#     # offsets for the top left corner of the crop\n",
    "#     offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
    "#     size = [-1, x2_shape[1], x2_shape[2], -1]\n",
    "#     x1_crop = tf.slice(x1, offsets, size)\n",
    "#     return tf.add(x1_crop, x2)\n",
    "# def crop_add_3rdchannel(x1,x2):\n",
    "#     x1_shape = tf.shape(x1)\n",
    "#     x2_shape = tf.shape(x2)\n",
    "#     # offsets for the top left corner of the crop\n",
    "#     offsets = [0, 0, 0, 2]\n",
    "#     size = [x2_shape[0], x2_shape[1], x2_shape[2], x2_shape[3]]\n",
    "#     x1_crop = tf.slice(x1, offsets, size)\n",
    "#     return tf.add(x1_crop, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unet2d:\n",
    "    def __init__(self):\n",
    "        self.input_shape = [256,256,7]\n",
    "        self.layers = 5\n",
    "        self.filter_size=3\n",
    "        self.pool_size=2\n",
    "        self.features_root=32\n",
    "        self.keep_prob=1.0\n",
    "        \n",
    "        self.dice_smooth = 1\n",
    "        \n",
    "        self.n_class=6\n",
    "        self.fc_nodes = [1024]\n",
    "        self.fc_drop = [0.25]\n",
    "        self.up_layers=0\n",
    "        self.finetune_scope = 'finetune'\n",
    "    \n",
    "    def add_to_parser(self, parser):\n",
    "        for key in vars(self):\n",
    "            value = vars(self)[key]\n",
    "            if type(value) == list:\n",
    "                parser.add_argument('--%s'%key, type=type(value[0]), nargs='+', default=value)\n",
    "            else:\n",
    "                parser.add_argument('--%s'%key, type=type(value), default=value)\n",
    "        return parser\n",
    "    \n",
    "    def from_args(self, args):\n",
    "        for key in vars(args):\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, getattr(args, key))\n",
    "    \n",
    "    def unet(self, X, phase):\n",
    "        # Build the encoder\n",
    "        layers = self.layers\n",
    "        filter_size = self.filter_size\n",
    "        pool_size = self.pool_size\n",
    "        features_root = self.features_root\n",
    "        keep_prob = self.keep_prob\n",
    "        channels = self.input_shape[-1]\n",
    "        \n",
    "        dw_h_convs = OrderedDict()\n",
    "        in_node = X\n",
    "        self.down_layer_vars = []\n",
    "        self.up_layer_vars = []\n",
    "\n",
    "        # down layers\n",
    "        for layer in range(0, layers):\n",
    "            features = 2**layer*features_root\n",
    "            stddev = np.sqrt(2 / (filter_size**2 * features))\n",
    "            if layer == 0:\n",
    "                w1 = weight_variable([filter_size, filter_size, channels, features])\n",
    "            else:\n",
    "                w1 = weight_variable([filter_size, filter_size, features//2, features])\n",
    "            w2 = weight_variable([filter_size, filter_size, features, features])\n",
    "            b1 = bias_variable([features])\n",
    "            b2 = bias_variable([features])\n",
    "            \n",
    "            self.down_layer_vars.append([w1, w2, b1, b2])\n",
    "\n",
    "            conv1 = conv2d(in_node, w1, keep_prob)\n",
    "            #tmp_h_conv = tf.nn.relu(conv1 + b1)\n",
    "            dw_h_convs[layer]=tf.nn.relu(batch_relu(conv1, phase,scope=\"down1_bn\"+str(layer)))\n",
    "\n",
    "            conv2 = conv2d_stride(dw_h_convs[layer], w2, keep_prob)\n",
    "            tmp_h_conv= tf.nn.relu(batch_relu(conv2, phase,scope=\"down2_bn\"+str(layer)))\n",
    "            \n",
    "            if layer < layers-1:\n",
    "                #pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
    "                in_node =tmp_h_conv\n",
    "\n",
    "        in_node = dw_h_convs[layers-1]  \n",
    "        # up layers\n",
    "        for layer in range(layers-2, layers-2-self.up_layers, -1):\n",
    "            features = 2**(layer+1)*features_root\n",
    "\n",
    "            wd = weight_variable_devonc([pool_size, pool_size, features//2, features])\n",
    "            bd = bias_variable([features//2])\n",
    "            h_deconv = tf.nn.relu(deconv2d(in_node, wd, pool_size) + bd)\n",
    "            h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)\n",
    "\n",
    "            w1 = weight_variable([filter_size, filter_size, features, features//2])\n",
    "            w2 = weight_variable([filter_size, filter_size, features//2, features//2])\n",
    "            b1 = bias_variable([features//2])\n",
    "            b2 = bias_variable([features//2])\n",
    "            \n",
    "            self.up_layer_vars.append([wd, bd, w1, w2, b1, b2])\n",
    "\n",
    "            conv1 = conv2d(h_deconv_concat, w1, keep_prob)\n",
    "            h_conv = tf.nn.relu(batch_relu(conv1, phase,scope=\"up1_bn\"+str(layer)))\n",
    "            conv2 = conv2d(h_conv, w2, keep_prob)\n",
    "            in_node = tf.nn.relu(batch_relu(conv2, phase,scope=\"up2_bn\"+str(layer)))\n",
    "        \n",
    "        return in_node\n",
    "\n",
    "    def build_unet(self):\n",
    "        self.X = tf.placeholder(\"float\", [None] + list(self.input_shape), name='X')\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.input_shape[0], self.input_shape[1], 1], name='Y')\n",
    "        self.phase = tf.placeholder(tf.bool, name='phase')\n",
    "        \n",
    "        self.up_layers = self.layers - 1\n",
    "        unet_output = self.unet(self.X, self.phase)\n",
    "        weight = weight_variable([1, 1, self.features_root, 1])\n",
    "        bias = bias_variable([1])\n",
    "        conv = conv2d(unet_output, weight, tf.constant(1.0))\n",
    "        self.pred_before_sigmoid = conv + bias\n",
    "        \n",
    "        self.pred = tf.keras.layers.Activation('sigmoid')(self.pred_before_sigmoid)\n",
    "        \n",
    "        self.dice_loss = 1 - (2 * tf.reduce_sum(self.Y * self.pred) + self.dice_smooth) / (tf.reduce_sum(self.Y) + tf.reduce_sum(self.pred) + self.dice_smooth)\n",
    "    \n",
    "    def build_unet_mask(self):\n",
    "        self.X = tf.placeholder(\"float\", [None] + list(self.input_shape), name='X')\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.input_shape[0], self.input_shape[1], 1], name='Y')\n",
    "        self.mask = tf.placeholder(\"float\", [None, self.input_shape[0], self.input_shape[1], 1], name='Y')\n",
    "        self.phase = tf.placeholder(tf.bool, name='phase')\n",
    "        \n",
    "        self.up_layers = self.layers - 1\n",
    "        unet_output = self.unet(self.X, self.phase)\n",
    "        weight = weight_variable([1, 1, self.features_root, 1])\n",
    "        bias = bias_variable([1])\n",
    "        conv = conv2d(unet_output, weight, tf.constant(1.0))\n",
    "        self.pred_before_sigmoid = conv + bias\n",
    "        \n",
    "        self.pred = tf.keras.layers.Activation('sigmoid')(self.pred_before_sigmoid) * self.mask\n",
    "        \n",
    "        self.dice_loss = 1 - (2 * tf.reduce_sum(self.Y * self.pred) + self.dice_smooth) / (tf.reduce_sum(self.Y) + tf.reduce_sum(self.pred) + self.dice_smooth)\n",
    "    \n",
    "    def build_unet_multiclass(self):\n",
    "        # tf Graph input (only pictures)\n",
    "        self.X = tf.placeholder(\"float\", [None] + list(self.input_shape), name='X')\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.input_shape[0], self.input_shape[1], self.n_class], name='Y')\n",
    "        self.phase = tf.placeholder(tf.bool, name='phase')\n",
    "        \n",
    "        self.up_layers = self.layers - 1\n",
    "        unet_output = self.unet(self.X, self.phase)\n",
    "        weight = weight_variable([1, 1, self.features_root, self.n_class])\n",
    "        bias = bias_variable([self.n_class])\n",
    "        conv = conv2d(unet_output, weight, tf.constant(1.0))\n",
    "        self.pred_before_softmax = conv + bias\n",
    "        \n",
    "        self.pred = tf.keras.layers.Activation('softmax')(self.pred_before_softmax)\n",
    "        \n",
    "        self.dices = 1 - (2 * tf.reduce_sum(self.Y[...,1:] * self.pred[...,1:], (1,2,3)) + self.dice_smooth) \\\n",
    "        / (tf.reduce_sum(self.Y[...,1:], (1,2,3)) + tf.reduce_sum(self.pred[...,1:], (1,2,3)) + self.dice_smooth)\n",
    "        \n",
    "        self.dice_loss = tf.reduce_mean(self.dices)\n",
    "    \n",
    "    def build_unet_multiclass_with_mask(self):\n",
    "        # tf Graph input (only pictures)\n",
    "        self.X = tf.placeholder(\"float\", [None] + list(self.input_shape), name='X')\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.input_shape[0], self.input_shape[1], self.n_class], name='Y')\n",
    "        self.mask = tf.placeholder(tf.float32, [None, self.input_shape[0], self.input_shape[1], 1], name='mask')\n",
    "        self.phase = tf.placeholder(tf.bool, name='phase')\n",
    "        \n",
    "        self.up_layers = self.layers - 1\n",
    "        unet_output = self.unet(self.X, self.phase)\n",
    "        weight = weight_variable([1, 1, self.features_root, self.n_class])\n",
    "        bias = bias_variable([self.n_class])\n",
    "        conv = conv2d(unet_output, weight, tf.constant(1.0))\n",
    "        self.pred_before_softmax = conv + bias\n",
    "        \n",
    "        \n",
    "        self.pred = tf.keras.layers.Activation('softmax')(self.pred_before_softmax)\n",
    "        \n",
    "        # output: always set the fore/background according to the mask\n",
    "        mask = tf.concat([1 - self.mask] + [self.mask]*(self.n_class-1), -1)\n",
    "        self.output_pred = tf.keras.layers.Activation('softmax')(self.pred_before_softmax) * mask\n",
    "        \n",
    "        \n",
    "        self.dices = 1 - (2 * tf.reduce_sum(self.Y[...,1:] * self.pred[...,1:], (1,2,3)) + self.dice_smooth) \\\n",
    "        / (tf.reduce_sum(self.Y[...,1:], (1,2,3)) + tf.reduce_sum(self.pred[...,1:], (1,2,3)) + self.dice_smooth)\n",
    "        \n",
    "        self.dice_loss = tf.reduce_mean(self.dices)\n",
    "    \n",
    "    def build_finetune(self):\n",
    "        # tf Graph input (only pictures)\n",
    "        self.X = tf.placeholder(\"float\", [None] + list(self.input_shape), name='X')\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.n_class], name='Y')\n",
    "        self.phase = tf.placeholder(tf.bool, name='phase')\n",
    "        \n",
    "        unet_output = self.unet(self.X, self.phase)\n",
    "        \n",
    "        # the output for classification\n",
    "        with tf.name_scope(self.finetune_scope):\n",
    "            self.features = tf.keras.layers.GlobalAveragePooling2D()(unet_output)\n",
    "            x = self.features\n",
    "            for fc_node, fc_drop in zip(self.fc_nodes, self.fc_drop):\n",
    "                x = tf.keras.layers.Dense(fc_node, 'relu')(x)\n",
    "                x = tf.keras.layers.Dropout(fc_drop)(x, self.phase)\n",
    "            self.pred_before_sigmoid = tf.keras.layers.Dense(self.n_class)(x)\n",
    "            self.pred = tf.keras.layers.Activation('sigmoid')(self.pred_before_sigmoid)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = self.Y, logits = self.pred_before_sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import subprocess\n",
    "    subprocess.call(['jupyter', 'nbconvert', '--to', 'script', 'unet2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
